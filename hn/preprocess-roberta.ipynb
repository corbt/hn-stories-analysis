{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4920000, 12)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "\n",
    "df = pd.read_feather('/workspace/data/hn/stories_dump.feather')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2126850, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'descendants': 'comments'}, inplace=True)\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "df['dead'] = df.dead.fillna(0).astype(bool)\n",
    "\n",
    "# Keep stories from 2018 onward in case community tastes have changed\n",
    "df = df[df['time'].dt.year > 2017]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1810290, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Only keep stories without text for now\n",
    "df = df[df['text'].isnull() & df['url'].notnull()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1544214, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this analysis we're trying to see whether a \"real\" story is likely to make\n",
    "# it to the front page. So let's remove dead stories as there are probably easier ways to detect those.\n",
    "df = df[df['dead'] == False]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1544214, 12)\n",
      "(1293918, 12)\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate stories based on the URL. Keep the one with the highest score.\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frontpage'] = (df.score >= 20) | (df.comments >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'comments', 'id', 'score', 'time', 'title', 'type', 'url', 'dead',\n",
       "       'text', 'kids', 'deleted', 'frontpage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f7c7826d38480f8317a5566d98d8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1293918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_text(row):\n",
    "  return f\"\"\"Title: {row.title}\n",
    "URL: {row.url}\n",
    "Poster: {row.by}\n",
    "Date: {row.time.strftime('%A, %B %d, %I:%M %p')}\"\"\"\n",
    "\n",
    "df['formatted_text'] = df.swifter.apply(format_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Amazon makes education push in India with JEE preparation app\n",
      "URL: https://techcrunch.com/2021/01/12/amazon-makes-education-push-in-india-with-jee-preparation-app/\n",
      "Poster: jmsflknr\n",
      "Date: Wednesday, January 13, 07:50 AM\n",
      "\n",
      "Title: If I could teach my younger programmer self a few things...\n",
      "URL: https://medium.com/@iluga/i-wish-i-knew-this-when-i-began-programming-7381b576c699\n",
      "Poster: danielwbean\n",
      "Date: Friday, August 28, 10:09 PM\n",
      "\n",
      "Title: Trealla – A compact, efficient Prolog interpreter written in plain-old C\n",
      "URL: https://github.com/trealla-prolog/trealla\n",
      "Poster: nikolay\n",
      "Date: Monday, August 08, 05:02 AM\n",
      "\n",
      "Title: Govt blocks Yahoo, Steam, PayPal for failing to comply with licensing policy\n",
      "URL: https://www.thejakartapost.com/indonesia/2022/07/30/govt-blocks-yahoo-steam-paypal-for-failing-to-comply-with-licensing-policy.html\n",
      "Poster: mfcc64\n",
      "Date: Saturday, July 30, 06:41 AM\n",
      "\n",
      "Title: What a hangover taught me about fake meritocracy\n",
      "URL: https://edmarferreira.com/archive/what-a-hangover-taught-me-about-fake-meritocracy/\n",
      "Poster: edmarferreira\n",
      "Date: Thursday, January 21, 06:00 PM\n",
      "\n",
      "Title: Regolith Linux r1.1-Ubuntu 19.04, international keyboard support, more polish\n",
      "URL: http://regolith-linux.org\n",
      "Poster: kgilmer\n",
      "Date: Tuesday, May 28, 02:45 PM\n",
      "\n",
      "Title: Sol – a sunny little virtual machine (2012)\n",
      "URL: https://rsms.me/sol-a-sunny-little-virtual-machine\n",
      "Poster: maastaar\n",
      "Date: Saturday, June 06, 07:52 AM\n",
      "\n",
      "Title: Microsoft Unveils Linux SQL Server 2022 Release Candidate\n",
      "URL: https://www.theregister.com/2022/09/08/microsoft_sql_server_linux_rc/\n",
      "Poster: metadat\n",
      "Date: Thursday, September 08, 10:48 PM\n",
      "\n",
      "Title: How to Fix Page Fault in Nonpaged Area BSOD Error on Windows 10\n",
      "URL: https://hackernoon.com/page-fault-in-nonpaged-area-bsod-error-on-windows-10-solved-986qg2ed9\n",
      "Poster: dlized\n",
      "Date: Monday, March 01, 07:55 PM\n",
      "\n",
      "Title: Marina Ovsyannikova: Search for journalist who protested war on Russian TV\n",
      "URL: https://www.bbc.co.uk/news/world-europe-60749279\n",
      "Poster: vanilla-almond\n",
      "Date: Tuesday, March 15, 01:11 PM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print 10 random stories\n",
    "for i, row in df.sample(10).iterrows():\n",
    "  print(row.formatted_text)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f03b11de16e4467871148fb855b96a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['by', 'comments', 'id', 'score', 'time', 'title', 'type', 'url', 'dead',\n",
       "       'text', 'kids', 'deleted', 'frontpage', 'formatted_text', 'input_ids',\n",
       "       'attention_mask'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    tokenizer_columns = tokenizer(chunk['formatted_text'].tolist(), padding=False, truncation=True, max_length=512)\n",
    "    for key in tokenizer_columns:\n",
    "        chunk[key] = tokenizer_columns[key]\n",
    "    return chunk\n",
    "\n",
    "# Split the dataset into batches of 1000 and apply the tokenizer columns to each batch\n",
    "chunks = np.array_split(df, ceil(df.shape[0]/1000))\n",
    "\n",
    "df = pd.concat([process_chunk(chunk) for chunk in tqdm(chunks)])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "test       64746\n",
       "train    1229172\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign 5% of the data to the test set randomly\n",
    "df['split'] = np.random.choice(['train', 'test'], p=[0.95, 0.05], size=(df.shape[0],))\n",
    "df = df.sample(frac=1, random_state=1318)\n",
    "\n",
    "df.groupby('split').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'comments', 'id', 'score', 'time', 'title', 'type', 'url', 'dead',\n",
       "       'text', 'kids', 'deleted', 'frontpage', 'formatted_text', 'input_ids',\n",
       "       'attention_mask', 'split'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels need to be a float for RMSE calculation\n",
    "df['labels'] = df['frontpage'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_feather('/workspace/data/hn/stories-roberta.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'input_ids', 'attention_mask', 'labels', 'split'],\n",
       "        num_rows: 1229172\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'input_ids', 'attention_mask', 'labels', 'split'],\n",
       "        num_rows: 64746\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "df = pd.read_feather('/workspace/data/hn/stories-roberta.feather')\n",
    "\n",
    "# Get just the columns we need\n",
    "df = df[['id', 'input_ids', 'attention_mask', 'labels', 'split']]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "  'train': Dataset.from_pandas(df[df['split'] == 'train'], preserve_index=False),\n",
    "  'test': Dataset.from_pandas(df[df['split'] == 'test'], preserve_index=False)\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc90386b4e24e8a8661c77f95477127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/1229172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adddf39c88934f7aa26fdf53d1bb63d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/64746 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk('/workspace/data/hn/stories-roberta-dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
