{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4920000, 12)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "\n",
    "df = pd.read_feather('/workspace/data/hn/stories_dump.feather')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2126850, 12)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'descendants': 'comments'}, inplace=True)\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "df['dead'] = df.dead.fillna(0).astype(bool)\n",
    "\n",
    "# Keep stories from 2018 onward in case community tastes have changed\n",
    "df = df[df['time'].dt.year > 2017]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1810290, 12)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Only keep stories without text for now\n",
    "df = df[df['text'].isnull() & df['url'].notnull()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1810290, 12)\n",
      "(1518276, 12)\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate stories based on the URL. Keep the one with the highest score.\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frontpage'] = (df.score >= 20) | (df.comments >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'comments', 'id', 'score', 'time', 'title', 'type', 'url', 'dead',\n",
       "       'text', 'kids', 'deleted', 'frontpage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5eeb227d9240188a06e464e492eae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1518276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_text(row):\n",
    "  return f\"\"\"Title: {row.title}\n",
    "URL: {row.url}\n",
    "Poster: {row.by}\n",
    "Date: {row.time.strftime('%A, %B %d, %I:%M %p')}\"\"\"\n",
    "\n",
    "df['formatted_text'] = df.swifter.apply(format_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Lossless-cut: The swiss army knife of lossless video/audio editing\n",
      "URL: https://github.com/mifi/lossless-cut\n",
      "Poster: tambourine_man\n",
      "Date: Sunday, October 25, 12:49 AM\n",
      "\n",
      "Title: DevOps Engineers – Skills of a Great Candidate\n",
      "URL: https://medium.com/@alexander.kainz/devops-engineers-6-skills-of-a-great-candidate-a22c92f96b0c\n",
      "Poster: soonnow\n",
      "Date: Sunday, May 17, 11:13 AM\n",
      "\n",
      "Title: NASA’s planet-hunting TESS telescope launches Monday aboard a SpaceX rocket\n",
      "URL: https://techcrunch.com/2018/04/14/nasas-planet-hunting-tess-telescope-launches-monday-aboard-a-spacex-rocket/\n",
      "Poster: Semirhage\n",
      "Date: Saturday, April 14, 08:20 PM\n",
      "\n",
      "Title: Researchers Scrutinize Optane Memory Performance\n",
      "URL: https://www.nextplatform.com/2019/03/18/researchers-scrutinize-optane-memory-performance/\n",
      "Poster: rbanffy\n",
      "Date: Tuesday, March 19, 10:51 AM\n",
      "\n",
      "Title: Bunnicula, asynchronous messaging with RabbitMQ for Clojure\n",
      "URL: https://blog.nomnominsights.com/bunnicula-asynchronous-messaging-with-rabbitmq-for-clojure/\n",
      "Poster: Plugawy\n",
      "Date: Friday, October 19, 08:48 AM\n",
      "\n",
      "Title: Tesla no longer allowing remote work\n",
      "URL: https://twitter.com/SamNissim/status/1531810291222192130\n",
      "Poster: edumucelli\n",
      "Date: Wednesday, June 01, 11:54 AM\n",
      "\n",
      "Title: A Central-Bank-Issued Digital Dollar Could Enable a Dark Future\n",
      "URL: https://www.city-journal.org/central-bank-digital-currency-caution\n",
      "Poster: noodles_nomore\n",
      "Date: Sunday, March 27, 06:10 PM\n",
      "\n",
      "Title: A Complete Essential Guide to Encrypting Your Data\n",
      "URL: https://technicalustad.com/a-complete-essential-guide-to-encrypting-your-data/\n",
      "Poster: balalrumy\n",
      "Date: Friday, November 02, 09:18 PM\n",
      "\n",
      "Title: L.A. Paul on Transformative Experiences and Our Future Selves\n",
      "URL: https://www.preposterousuniverse.com/podcast/2020/02/24/85-l-a-paul-on-transformative-experiences-and-our-future-selves/\n",
      "Poster: simonebrunozzi\n",
      "Date: Tuesday, February 25, 05:42 PM\n",
      "\n",
      "Title: What the Non-Technical Entrepreneur Needs to Know About Tech\n",
      "URL: https://altar.io/blog/what-the-non-technical-entrepreneur-needs-to-know-about-tech/\n",
      "Poster: Rui_Lou\n",
      "Date: Saturday, December 07, 12:14 PM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print 10 random stories\n",
    "for i, row in df.sample(10).iterrows():\n",
    "  print(row.formatted_text)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    tokenizer_columns = tokenizer(chunk['formatted_text'].tolist(), padding=False, truncation=True, max_length=512)\n",
    "    for key in tokenizer_columns:\n",
    "        chunk[key] = tokenizer_columns[key]\n",
    "    return chunk\n",
    "\n",
    "# Split the dataset into batches of 1000 and apply the tokenizer columns to each batch\n",
    "chunks = np.array_split(df, ceil(df.shape[0]/1000))\n",
    "\n",
    "df = pd.concat([process_chunk(chunk) for chunk in tqdm(chunks)])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_feather('/workspace/data/hn/stories-tokenized.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'time', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1442362\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'time', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 75914\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "df = pd.read_feather('/workspace/data/hn/stories-tokenized.feather')\n",
    "\n",
    "# Labels need to be a float for RMSE calculation\n",
    "df['labels'] = df['frontpage'].astype(float)\n",
    "\n",
    "# Get just the columns we need\n",
    "df = df[['id', 'time', 'input_ids', 'attention_mask', 'labels']]\n",
    "\n",
    "# Split data into train and test based on publication date.\n",
    "df = df.sort_values('time')\n",
    "\n",
    "split_date = df.iloc[int(len(df) * 0.95)]['time']\n",
    "\n",
    "train_df = df[df['time'] < split_date]\n",
    "test_df = df[df['time'] >= split_date]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "  'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
    "  'test': Dataset.from_pandas(test_df, preserve_index=False),\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97edeb7c06df4d0ba119fa83deed0a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/1442362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dc1d3d95b545578509fa971bb6e8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/75914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk('/workspace/data/hn/stories-dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
