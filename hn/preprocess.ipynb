{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4920000, 12)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "\n",
    "df = pd.read_feather('/workspace/data/hn/stories_dump.feather')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2126850, 12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'descendants': 'comments'}, inplace=True)\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "df['dead'] = df.dead.fillna(0).astype(bool)\n",
    "\n",
    "# Keep stories from 2018 onward in case community tastes have changed\n",
    "df = df[df['time'].dt.year > 2017]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1810290, 12)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Only keep stories without text for now\n",
    "df = df[df['text'].isnull() & df['url'].notnull()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1810290, 12)\n",
      "(1518276, 12)\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate stories based on the URL. Keep the one with the highest score.\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frontpage'] = (df.score >= 20) | (df.comments >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'comments', 'id', 'score', 'time', 'title', 'type', 'url', 'dead',\n",
       "       'text', 'kids', 'deleted', 'frontpage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8942ae2aef436e9bf022c7f9668c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1518276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_text(row):\n",
    "  return f\"\"\"Title: {row.title}\n",
    "URL: {row.url}\n",
    "Poster: {row.by}\n",
    "Date: {row.time.strftime('%A, %B %d, %I:%M %p')}\"\"\"\n",
    "\n",
    "df['formatted_text'] = df.swifter.apply(format_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a85c74175674797958535bd3d28630d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6513285f4029460f96ec2abba803b316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0405ce53e0354e6f8d3cd7fc9d9c46fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0007f63da6a3429f9772ea3e4b6815b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['by', 'comments', 'id', 'score', 'time', 'title', 'type', 'url', 'dead',\n",
       "       'text', 'kids', 'deleted', 'frontpage', 'formatted_text', 'input_ids',\n",
       "       'token_type_ids', 'attention_mask'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    tokenizer_columns = tokenizer(chunk['formatted_text'].tolist(), padding=False, truncation=True, max_length=512)\n",
    "    for key in tokenizer_columns:\n",
    "        chunk[key] = tokenizer_columns[key]\n",
    "    return chunk\n",
    "\n",
    "# Split the dataset into batches of 1000 and apply the tokenizer columns to each batch\n",
    "chunks = np.array_split(df, ceil(df.shape[0]/1000))\n",
    "\n",
    "df = pd.concat([process_chunk(chunk) for chunk in tqdm(chunks)])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_feather('/workspace/data/hn/stories-tokenized.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'time', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1442362\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'time', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 75914\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "df = pd.read_feather('/workspace/data/hn/stories-tokenized.feather')\n",
    "\n",
    "# Labels need to be a float for RMSE calculation\n",
    "df['labels'] = df['frontpage'].astype(float)\n",
    "\n",
    "# Get just the columns we need\n",
    "df = df[['id', 'time', 'input_ids', 'attention_mask', 'labels']]\n",
    "\n",
    "# Split data into train and test based on publication date.\n",
    "df = df.sort_values('time')\n",
    "\n",
    "split_date = df.iloc[int(len(df) * 0.95)]['time']\n",
    "\n",
    "train_df = df[df['time'] < split_date]\n",
    "test_df = df[df['time'] >= split_date]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "  'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
    "  'test': Dataset.from_pandas(test_df, preserve_index=False),\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaec77fa5b9d4c8d97e93f483cf2b72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/1442362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10f7e22f69549c287fe821770d70de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/75914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk('/workspace/data/hn/stories-dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
