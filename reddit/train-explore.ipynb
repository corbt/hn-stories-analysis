{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.4)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
      "Requirement already satisfied: protobuf==3.20 in /usr/local/lib/python3.10/dist-packages (3.20.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.25.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.8.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.24.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu117)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.1)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (15.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iterative-stratification) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.2.1)\n",
      "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 xxhash-3.2.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb python-dotenv iterative-stratification accelerate protobuf==3.20 sentencepiece tensorboardX datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel, AutoConfig, \n",
    "    AutoTokenizer, logging,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import pandas as pd\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"microsoft/deberta-v3-base\",# \"distilbert-base-uncased\",\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"dropout\": random.uniform(0.01, 0.60),\n",
    "    \"max_length\": 512,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"valid_batch_size\": 16,\n",
    "    \"epochs\": 10,\n",
    "    \"folds\" : 3,\n",
    "    \"max_grad_norm\": 1000,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"learning_rate\": 1e-5,\n",
    "     \"loss_type\": \"rmse\",\n",
    "    \"n_accumulate\" : 1,\n",
    "    \"label_cols\" : ['upvote_ratio', 'log_score'],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'subreddit', 'title', 'selftext', 'created_utc',\n",
       "       'score', 'upvote_ratio', 'removed_by_category', 'log_score',\n",
       "       'formatted_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_feather('/workspace/data/reddit/submissions/RS_2023-01-train.arrow')\n",
    "test = pd.read_feather('/workspace/data/reddit/submissions/RS_2023-01-test.arrow')\n",
    "\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomIterator(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, labels=CONFIG['label_cols'], is_train=True):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = CONFIG[\"max_length\"]# tokenizer.model_max_length\n",
    "        self.labels = labels\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        tokens = self.tokenizer(\n",
    "                    self.df.loc[idx, 'formatted_text'],#.to_list(),\n",
    "                    add_special_tokens=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=self.max_seq_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt',\n",
    "                    return_attention_mask=True\n",
    "                )     \n",
    "        res = {\n",
    "            'input_ids': tokens['input_ids'].to(CONFIG.get('device')).squeeze(),\n",
    "            'attention_mask': tokens['attention_mask'].to(CONFIG.get('device')).squeeze()\n",
    "        }\n",
    "        \n",
    "        if self.is_train:\n",
    "            res[\"labels\"] = torch.tensor(\n",
    "                self.df.loc[idx, self.labels].to_list(), \n",
    "            ).to(CONFIG.get('device')) \n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "      \n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()  \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.hidden_dropout_prob = 0\n",
    "        self.config.attention_probs_dropout_prob = 0\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, len(CONFIG['label_cols']))\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids,\n",
    "                         attention_mask=attention_mask, \n",
    "                         output_hidden_states=False)\n",
    "        out = self.pooler(out.last_hidden_state, attention_mask)\n",
    "        out = self.drop(out)\n",
    "        outputs = self.fc(out)\n",
    "        return SequenceClassifierOutput(logits=outputs)\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Code taken from Y Nakama's notebook (https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = torch.sqrt(self.mse(predictions, targets) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        loss_func = RMSELoss(reduction='mean')\n",
    "        loss = loss_func(outputs.logits.float(), inputs['labels'].float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    colwise_rmse = np.sqrt(np.mean((labels - predictions) ** 2, axis=0))\n",
    "    res = {\n",
    "        f\"{analytic.upper()}_RMSE\" : colwise_rmse[i]\n",
    "        for i, analytic in enumerate(CONFIG[\"label_cols\"])\n",
    "    }\n",
    "    res[\"MCRMSE\"] = np.mean(colwise_rmse)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquixote\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb, random, os\n",
    "\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to produce similar folds\n",
    "SEED = 1318\n",
    "cv = MultilabelStratifiedKFold(n_splits=CONFIG.get(\"folds\", 3), shuffle=True, random_state=SEED)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "for fold, ( _, val_idx) in enumerate(cv.split(X=train, y=train[CONFIG['label_cols']])):\n",
    "    train.loc[val_idx , \"fold\"] = int(fold)\n",
    "    \n",
    "train[\"fold\"] = train[\"fold\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs-{fold}/\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=CONFIG['train_batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['valid_batch_size'],\n",
    "        num_train_epochs=CONFIG['epochs'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay'],\n",
    "        gradient_accumulation_steps=CONFIG['n_accumulate'],\n",
    "        seed=SEED,\n",
    "        group_by_length=True,\n",
    "        max_grad_norm=CONFIG['max_grad_norm'],\n",
    "        metric_for_best_model='eval_MCRMSE',\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=False,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        report_to=\"wandb\",\n",
    "        label_names=[\"labels\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- Fold: 0 ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/reddit/wandb/run-20230615_051127-hnhta1or</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quixote/FB3-deberta-v3/runs/hnhta1or' target=\"_blank\">FB3-fold-0</a></strong> to <a href='https://wandb.ai/quixote/FB3-deberta-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quixote/FB3-deberta-v3' target=\"_blank\">https://wandb.ai/quixote/FB3-deberta-v3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quixote/FB3-deberta-v3/runs/hnhta1or' target=\"_blank\">https://wandb.ai/quixote/FB3-deberta-v3/runs/hnhta1or</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f301e116917448a5b6087735c0e5d182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m\n\u001b[1;32m     48\u001b[0m trainer \u001b[39m=\u001b[39m CustomTrainer(\n\u001b[1;32m     49\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     50\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[39m# LAUNCH THE TRAINER\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1659\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1657\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrently training with a batch size of: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1658\u001b[0m \u001b[39m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1659\u001b[0m train_dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_train_dataloader()\n\u001b[1;32m   1661\u001b[0m \u001b[39m# Setting up training control variables:\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[39m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m \u001b[39m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \u001b[39m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m total_train_batch_size \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mtrain_batch_size \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mworld_size\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:919\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    903\u001b[0m         train_dataset \u001b[39m=\u001b[39m IterableDatasetShard(\n\u001b[1;32m    904\u001b[0m             train_dataset,\n\u001b[1;32m    905\u001b[0m             batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m             process_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mprocess_index,\n\u001b[1;32m    909\u001b[0m         )\n\u001b[1;32m    911\u001b[0m     \u001b[39mreturn\u001b[39;00m DataLoader(\n\u001b[1;32m    912\u001b[0m         train_dataset,\n\u001b[1;32m    913\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m         pin_memory\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_pin_memory,\n\u001b[1;32m    917\u001b[0m     )\n\u001b[0;32m--> 919\u001b[0m train_sampler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_train_sampler()\n\u001b[1;32m    921\u001b[0m \u001b[39mreturn\u001b[39;00m DataLoader(\n\u001b[1;32m    922\u001b[0m     train_dataset,\n\u001b[1;32m    923\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     worker_init_fn\u001b[39m=\u001b[39mseed_worker,\n\u001b[1;32m    930\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:841\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m model_input_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[39mreturn\u001b[39;00m LengthGroupedSampler(\n\u001b[1;32m    842\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mtrain_batch_size \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mgradient_accumulation_steps,\n\u001b[1;32m    843\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataset,\n\u001b[1;32m    844\u001b[0m         lengths\u001b[39m=\u001b[39;49mlengths,\n\u001b[1;32m    845\u001b[0m         model_input_name\u001b[39m=\u001b[39;49mmodel_input_name,\n\u001b[1;32m    846\u001b[0m         generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m    847\u001b[0m     )\n\u001b[1;32m    848\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     \u001b[39mreturn\u001b[39;00m DistributedLengthGroupedSampler(\n\u001b[1;32m    850\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps,\n\u001b[1;32m    851\u001b[0m         dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m         seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m    857\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py:578\u001b[0m, in \u001b[0;36mLengthGroupedSampler.__init__\u001b[0;34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    571\u001b[0m         \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(dataset[\u001b[39m0\u001b[39m], \u001b[39mdict\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset[\u001b[39m0\u001b[39m], BatchEncoding))\n\u001b[1;32m    572\u001b[0m         \u001b[39mor\u001b[39;00m model_input_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m dataset[\u001b[39m0\u001b[39m]\n\u001b[1;32m    573\u001b[0m     ):\n\u001b[1;32m    574\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_input_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m key.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         )\n\u001b[0;32m--> 578\u001b[0m     lengths \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(feature[model_input_name]) \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m dataset]\n\u001b[1;32m    579\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(lengths, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    580\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    581\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py:578\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    571\u001b[0m         \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(dataset[\u001b[39m0\u001b[39m], \u001b[39mdict\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset[\u001b[39m0\u001b[39m], BatchEncoding))\n\u001b[1;32m    572\u001b[0m         \u001b[39mor\u001b[39;00m model_input_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m dataset[\u001b[39m0\u001b[39m]\n\u001b[1;32m    573\u001b[0m     ):\n\u001b[1;32m    574\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_input_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m key.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         )\n\u001b[0;32m--> 578\u001b[0m     lengths \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(feature[model_input_name]) \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m dataset]\n\u001b[1;32m    579\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(lengths, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    580\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    581\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36mCustomIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m res \u001b[39m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: tokens[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(CONFIG\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39msqueeze(),\n\u001b[1;32m     21\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: tokens[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(CONFIG\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_train:\n\u001b[1;32m     25\u001b[0m     res[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49mloc[idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels]\u001b[39m.\u001b[39mto_list(), \n\u001b[1;32m     27\u001b[0m     )\u001b[39m.\u001b[39mto(CONFIG\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m)) \n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1097\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1096\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1097\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1098\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1280\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1279\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_lowerdim(tup)\n\u001b[1;32m   1282\u001b[0m \u001b[39m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1024\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[39mreturn\u001b[39;00m section\n\u001b[1;32m   1023\u001b[0m         \u001b[39m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(section, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)[new_key]\n\u001b[1;32m   1026\u001b[0m \u001b[39mraise\u001b[39;00m IndexingError(\u001b[39m\"\u001b[39m\u001b[39mnot applicable\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1332\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1330\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1334\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1272\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1271\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1273\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1274\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1462\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1459\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1460\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1462\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1464\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:5871\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5868\u001b[0m     keyarr \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   5870\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 5871\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer_for(keyarr)\n\u001b[1;32m   5872\u001b[0m     keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(keyarr)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:5858\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   5840\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5841\u001b[0m \u001b[39mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   5842\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5855\u001b[0m \u001b[39marray([0, 2])\u001b[39;00m\n\u001b[1;32m   5856\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 5858\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer(target)\n\u001b[1;32m   5859\u001b[0m indexer, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   5860\u001b[0m \u001b[39mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:3726\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3724\u001b[0m method \u001b[39m=\u001b[39m clean_reindex_fill_method(method)\n\u001b[1;32m   3725\u001b[0m orig_target \u001b[39m=\u001b[39m target\n\u001b[0;32m-> 3726\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_cast_listlike_indexer(target)\n\u001b[1;32m   3728\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3730\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:6362\u001b[0m, in \u001b[0;36mIndex._maybe_cast_listlike_indexer\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_cast_listlike_indexer\u001b[39m(\u001b[39mself\u001b[39m, target) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[1;32m   6359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   6360\u001b[0m \u001b[39m    Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\u001b[39;00m\n\u001b[1;32m   6361\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6362\u001b[0m     \u001b[39mreturn\u001b[39;00m ensure_index(target)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:7125\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7123\u001b[0m         \u001b[39mreturn\u001b[39;00m Index(index_like, copy\u001b[39m=\u001b[39mcopy, tupleize_cols\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   7124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 7125\u001b[0m     \u001b[39mreturn\u001b[39;00m Index(index_like, copy\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:551\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    548\u001b[0m         data \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(data, dtype\u001b[39m=\u001b[39m_dtype_obj)\n\u001b[1;32m    550\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     arr \u001b[39m=\u001b[39m sanitize_array(data, \u001b[39mNone\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    552\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    553\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mindex must be specified when data is not list-like\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/construction.py:569\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    567\u001b[0m subarr \u001b[39m=\u001b[39m data\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m--> 569\u001b[0m     subarr \u001b[39m=\u001b[39m maybe_infer_to_datetimelike(data)\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m subarr \u001b[39mis\u001b[39;00m data \u001b[39mand\u001b[39;00m copy:\n\u001b[1;32m    572\u001b[0m     subarr \u001b[39m=\u001b[39m subarr\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/cast.py:1189\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[1;32m   1186\u001b[0m \u001b[39m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmaybe_convert_objects(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1190\u001b[0m     value,\n\u001b[1;32m   1191\u001b[0m     \u001b[39m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;49;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[39m#  numpy would have done it for us.\u001b[39;49;00m\n\u001b[1;32m   1193\u001b[0m     convert_numeric\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1194\u001b[0m     convert_period\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1195\u001b[0m     convert_interval\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1196\u001b[0m     convert_timedelta\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1197\u001b[0m     convert_datetime\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1198\u001b[0m     dtype_if_all_nat\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mdtype(\u001b[39m\"\u001b[39;49m\u001b[39mM8[ns]\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1199\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx:2445\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/numeric.py:345\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[1;32m    343\u001b[0m     dtype \u001b[39m=\u001b[39m fill_value\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    344\u001b[0m a \u001b[39m=\u001b[39m empty(shape, dtype, order)\n\u001b[0;32m--> 345\u001b[0m multiarray\u001b[39m.\u001b[39;49mcopyto(a, fill_value, casting\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39munsafe\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    346\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mcopyto\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data Collator for Dynamic Padding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'], use_fast=True)\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# init predictions by fold\n",
    "predictions = {}\n",
    "for fold in range(0, CONFIG['folds']):\n",
    "    print(f\" ---- Fold: {fold} ----\")\n",
    "    run = wandb.init(project=\"FB3-deberta-v3\", \n",
    "                     config=CONFIG,\n",
    "                     job_type='train',\n",
    "                     group=\"FB3-BASELINE-MODEL\",\n",
    "                     tags=[CONFIG['model_name'], CONFIG['loss_type'], \"10-epochs\"],\n",
    "                     name=f'FB3-fold-{fold}',\n",
    "                     anonymous='must')\n",
    "    # the reset index is VERY IMPORTANT for the Dataset iterator\n",
    "    df_train = train[train.fold != fold].reset_index(drop=True)\n",
    "    df_valid = train[train.fold == fold].reset_index(drop=True)\n",
    "    # create iterators\n",
    "    train_dataset = CustomIterator(df_train, tokenizer)\n",
    "    valid_dataset = CustomIterator(df_valid, tokenizer)\n",
    "    # init model\n",
    "    model = FeedBackModel(CONFIG['model_name'])\n",
    "    model.to(CONFIG['device'])\n",
    "    \n",
    "    # SET THE OPITMIZER AND THE SCHEDULER\n",
    "    # no decay for bias and normalization layers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": CONFIG['weight_decay'],\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CONFIG['learning_rate'])\n",
    "    num_training_steps = (len(train_dataset) * CONFIG['epochs']) // (CONFIG['train_batch_size'] * CONFIG['n_accumulate'])\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.1*num_training_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    # CREATE THE TRAINER\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=collate_fn,\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    # LAUNCH THE TRAINER\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
