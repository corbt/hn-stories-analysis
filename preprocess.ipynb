{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
      "Collecting swifter\n",
      "  Downloading swifter-1.3.5.tar.gz (490 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.6/490.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from swifter) (5.9.5)\n",
      "Collecting dask[dataframe]>=2.10.0 (from swifter)\n",
      "  Downloading dask-2023.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (8.0.6)\n",
      "Collecting cloudpickle>=0.2.2 (from swifter)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: parso>0.4.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (0.8.3)\n",
      "Requirement already satisfied: bleach>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from swifter) (6.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from bleach>=3.1.1->swifter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (8.1.3)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (23.1)\n",
      "Collecting partd>=1.2.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading partd-1.4.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.0)\n",
      "Collecting toolz>=0.10.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata>=4.13.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter) (6.23.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.6)\n",
      "Requirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.3.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (4.8.0)\n",
      "Collecting locket (from partd>=1.2.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (3.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.2)\n",
      "Building wheels for collected packages: swifter\n",
      "  Building wheel for swifter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for swifter: filename=swifter-1.3.5-py3-none-any.whl size=16518 sha256=d619bdb241d90d38c5a42028a012ca56ea23cbd3c9a8b4d935349fd999a2dd06\n",
      "  Stored in directory: /root/.cache/pip/wheels/00/7e/8c/438ea309a16ef1a9130849060f962e0d2c4e94b8a4314d5919\n",
      "Successfully built swifter\n",
      "Installing collected packages: toolz, locket, importlib-metadata, cloudpickle, partd, dask, swifter\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.6.4\n",
      "    Uninstalling importlib-metadata-4.6.4:\n",
      "      Successfully uninstalled importlib-metadata-4.6.4\n",
      "Successfully installed cloudpickle-2.2.1 dask-2023.6.0 importlib-metadata-6.6.0 locket-1.0.0 partd-1.4.0 swifter-1.3.5 toolz-0.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install zstandard pandas tqdm swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import swifter\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 100000 lines (0%) (0 failed)\n",
      "Processed 200000 lines (1%) (0 failed)\n",
      "Processed 300000 lines (1%) (0 failed)\n",
      "Processed 400000 lines (1%) (0 failed)\n",
      "Processed 500000 lines (1%) (0 failed)\n",
      "Processed 600000 lines (2%) (0 failed)\n",
      "Processed 700000 lines (2%) (0 failed)\n",
      "Processed 800000 lines (2%) (0 failed)\n",
      "Processed 900000 lines (3%) (0 failed)\n",
      "Processed 1000000 lines (3%) (0 failed)\n",
      "Processed 1100000 lines (3%) (0 failed)\n",
      "Processed 1200000 lines (3%) (0 failed)\n",
      "Processed 1300000 lines (4%) (0 failed)\n",
      "Processed 1400000 lines (4%) (0 failed)\n",
      "Processed 1500000 lines (4%) (0 failed)\n",
      "Processed 1600000 lines (4%) (0 failed)\n",
      "Processed 1700000 lines (5%) (0 failed)\n",
      "Processed 1800000 lines (5%) (0 failed)\n",
      "Processed 1900000 lines (5%) (0 failed)\n",
      "Processed 2000000 lines (6%) (0 failed)\n",
      "Processed 2100000 lines (6%) (0 failed)\n",
      "Processed 2200000 lines (6%) (0 failed)\n",
      "Processed 2300000 lines (6%) (0 failed)\n",
      "Processed 2400000 lines (7%) (0 failed)\n",
      "Processed 2500000 lines (7%) (0 failed)\n",
      "Processed 2600000 lines (7%) (0 failed)\n",
      "Processed 2700000 lines (7%) (0 failed)\n",
      "Processed 2800000 lines (8%) (0 failed)\n",
      "Processed 2900000 lines (8%) (0 failed)\n",
      "Processed 3000000 lines (8%) (0 failed)\n",
      "Processed 3100000 lines (9%) (0 failed)\n",
      "Processed 3200000 lines (9%) (0 failed)\n",
      "Processed 3300000 lines (9%) (0 failed)\n",
      "Processed 3400000 lines (9%) (0 failed)\n",
      "Processed 3500000 lines (10%) (0 failed)\n",
      "Processed 3600000 lines (10%) (0 failed)\n",
      "Processed 3700000 lines (10%) (0 failed)\n",
      "Processed 3800000 lines (10%) (0 failed)\n",
      "Processed 3900000 lines (11%) (0 failed)\n",
      "Processed 4000000 lines (11%) (0 failed)\n",
      "Processed 4100000 lines (11%) (0 failed)\n",
      "Processed 4200000 lines (12%) (0 failed)\n",
      "Processed 4300000 lines (12%) (0 failed)\n",
      "Processed 4400000 lines (12%) (0 failed)\n",
      "Processed 4500000 lines (12%) (0 failed)\n",
      "Processed 4600000 lines (13%) (0 failed)\n",
      "Processed 4700000 lines (13%) (0 failed)\n",
      "Processed 4800000 lines (13%) (0 failed)\n",
      "Processed 4900000 lines (13%) (0 failed)\n",
      "Processed 5000000 lines (14%) (0 failed)\n",
      "Processed 5100000 lines (14%) (0 failed)\n",
      "Processed 5200000 lines (14%) (0 failed)\n",
      "Processed 5300000 lines (15%) (0 failed)\n",
      "Processed 5400000 lines (15%) (0 failed)\n",
      "Processed 5500000 lines (15%) (0 failed)\n",
      "Processed 5600000 lines (15%) (0 failed)\n",
      "Processed 5700000 lines (16%) (0 failed)\n",
      "Processed 5800000 lines (16%) (0 failed)\n",
      "Processed 5900000 lines (16%) (0 failed)\n",
      "Processed 6000000 lines (17%) (0 failed)\n",
      "Processed 6100000 lines (17%) (0 failed)\n",
      "Processed 6200000 lines (17%) (0 failed)\n",
      "Processed 6300000 lines (17%) (0 failed)\n",
      "Processed 6400000 lines (18%) (0 failed)\n",
      "Processed 6500000 lines (18%) (0 failed)\n",
      "Processed 6600000 lines (18%) (0 failed)\n",
      "Processed 6700000 lines (19%) (0 failed)\n",
      "Processed 6800000 lines (19%) (0 failed)\n",
      "Processed 6900000 lines (19%) (0 failed)\n",
      "Processed 7000000 lines (19%) (0 failed)\n",
      "Processed 7100000 lines (20%) (0 failed)\n",
      "Processed 7200000 lines (20%) (0 failed)\n",
      "Processed 7300000 lines (20%) (0 failed)\n",
      "Processed 7400000 lines (20%) (0 failed)\n",
      "Processed 7500000 lines (21%) (0 failed)\n",
      "Processed 7600000 lines (21%) (0 failed)\n",
      "Processed 7700000 lines (21%) (0 failed)\n",
      "Processed 7800000 lines (22%) (0 failed)\n",
      "Processed 7900000 lines (22%) (0 failed)\n",
      "Processed 8000000 lines (22%) (0 failed)\n",
      "Processed 8100000 lines (22%) (0 failed)\n",
      "Processed 8200000 lines (23%) (0 failed)\n",
      "Decoding error with 134,217,728 bytes, reading another chunk\n",
      "Processed 8300000 lines (23%) (0 failed)\n",
      "Processed 8400000 lines (23%) (0 failed)\n",
      "Processed 8500000 lines (23%) (0 failed)\n",
      "Processed 8600000 lines (24%) (0 failed)\n",
      "Processed 8700000 lines (24%) (0 failed)\n",
      "Processed 8800000 lines (24%) (0 failed)\n",
      "Processed 8900000 lines (25%) (0 failed)\n",
      "Processed 9000000 lines (25%) (0 failed)\n",
      "Processed 9100000 lines (25%) (0 failed)\n",
      "Processed 9200000 lines (25%) (0 failed)\n",
      "Processed 9300000 lines (26%) (0 failed)\n",
      "Processed 9400000 lines (26%) (0 failed)\n",
      "Processed 9500000 lines (26%) (0 failed)\n",
      "Processed 9600000 lines (27%) (0 failed)\n",
      "Processed 9700000 lines (27%) (0 failed)\n",
      "Processed 9800000 lines (27%) (0 failed)\n",
      "Processed 9900000 lines (27%) (0 failed)\n",
      "Processed 10000000 lines (28%) (0 failed)\n",
      "Processed 10100000 lines (28%) (0 failed)\n",
      "Processed 10200000 lines (28%) (0 failed)\n",
      "Processed 10300000 lines (28%) (0 failed)\n",
      "Processed 10400000 lines (29%) (0 failed)\n",
      "Processed 10500000 lines (29%) (0 failed)\n",
      "Processed 10600000 lines (29%) (0 failed)\n",
      "Processed 10700000 lines (30%) (0 failed)\n",
      "Processed 10800000 lines (30%) (0 failed)\n",
      "Processed 10900000 lines (30%) (0 failed)\n",
      "Processed 11000000 lines (30%) (0 failed)\n",
      "Processed 11100000 lines (31%) (0 failed)\n",
      "Processed 11200000 lines (31%) (0 failed)\n",
      "Processed 11300000 lines (31%) (0 failed)\n",
      "Processed 11400000 lines (32%) (0 failed)\n",
      "Processed 11500000 lines (32%) (0 failed)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m   log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessed \u001b[39m\u001b[39m{\u001b[39;00mfile_lines\u001b[39m}\u001b[39;00m\u001b[39m lines (\u001b[39m\u001b[39m{\u001b[39;00m(file_bytes_processed\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mfile_size)\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%) (\u001b[39m\u001b[39m{\u001b[39;00mbad_lines\u001b[39m}\u001b[39;00m\u001b[39m failed)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m   parsed \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(line)\n\u001b[1;32m     77\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parsed[\u001b[39m'\u001b[39m\u001b[39mselftext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Adapted from https://github.com/Watchful1/PushshiftDumps/blob/master/scripts/single_file.py\n",
    "\n",
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import logging.handlers\n",
    "\n",
    "\n",
    "log = logging.getLogger(\"bot\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
    "\tchunk = reader.read(chunk_size)\n",
    "\tbytes_read += chunk_size\n",
    "\tif previous_chunk is not None:\n",
    "\t\tchunk = previous_chunk + chunk\n",
    "\ttry:\n",
    "\t\treturn chunk.decode()\n",
    "\texcept UnicodeDecodeError:\n",
    "\t\tif bytes_read > max_window_size:\n",
    "\t\t\traise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
    "\t\tlog.info(f\"Decoding error with {bytes_read:,} bytes, reading another chunk\")\n",
    "\t\treturn read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
    "\n",
    "\n",
    "def read_lines_zst(file_name):\n",
    "\twith open(file_name, 'rb') as file_handle:\n",
    "\t\tbuffer = ''\n",
    "\t\treader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "\t\twhile True:\n",
    "\t\t\tchunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
    "\n",
    "\t\t\tif not chunk:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tlines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "\t\t\tfor line in lines[:-1]:\n",
    "\t\t\t\tyield line, file_handle.tell()\n",
    "\n",
    "\t\t\tbuffer = lines[-1]\n",
    "\n",
    "\t\treader.close()\n",
    "\n",
    "\n",
    "in_file = '/workspace/data/reddit/submissions/RS_2023-01.zst'\n",
    "out_file = '/workspace/data/reddit/submissions/RS_2023-01.jsonl'\n",
    "\n",
    "fields = [\n",
    "  'id',\n",
    "  'author',\n",
    "  'subreddit',\n",
    "  'title',\n",
    "  'selftext',\n",
    "  'created_utc',\n",
    "  'score',\n",
    "  'upvote_ratio',\n",
    "  'removed_by_category',\n",
    "  'num_comments',\n",
    "]\n",
    "\n",
    "file_size = os.stat(in_file).st_size\n",
    "file_lines = 0\n",
    "file_bytes_processed = 0\n",
    "created = None\n",
    "bad_lines = 0\n",
    "\n",
    "with open(out_file, 'w') as out:\n",
    "  for line, file_bytes_processed in read_lines_zst(in_file):    \n",
    "    file_lines += 1\n",
    "    if file_lines % 100000 == 0:\n",
    "      log.info(f\"Processed {file_lines} lines ({(file_bytes_processed / file_size) * 100:.0f}%) ({bad_lines} failed)\")\n",
    "\n",
    "    try:\n",
    "      parsed = json.loads(line)\n",
    "\n",
    "      if len(parsed['selftext'] or '') < 10:\n",
    "        continue\n",
    "      \n",
    "      # Only keep the fields we want\n",
    "      obj = {k: parsed[k] for k in fields}\n",
    "      \n",
    "      out.write(json.dumps(obj) + '\\n')\n",
    "    except (KeyError, json.JSONDecodeError) as err:\n",
    "      print(err)\n",
    "\n",
    "log.info(f\"Complete : {file_lines:,} : {bad_lines:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(out_file, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather('/workspace/data/reddit/submissions/RS_2023-01.arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('/workspace/data/reddit/submissions/RS_2023-01.arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380634, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit to only subreddits with at least 1000 submissions\n",
    "df = df.groupby('subreddit').filter(lambda x: len(x) > 1000)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_score'] = np.log10(df['score'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0494768cc04334adaa1f44960a35e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2380634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_text(row):\n",
    "  return f\"\"\"Title: {row.title}\n",
    "Subreddit: /r/{row.subreddit}\n",
    "Author: /u/{row.author}\n",
    "Posted: {row.created_utc.strftime('%A, %B %d, %I:%M %p')}\n",
    "\n",
    "Text: {row.selftext}\"\"\"\n",
    "\n",
    "df['formatted_text'] = df.swifter.apply(format_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1904507, 11), (476127, 11))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train and test based on date. 80% train, 20% test\n",
    "\n",
    "df = df.sort_values('created_utc')\n",
    "\n",
    "split_date = df.iloc[int(len(df) * 0.8)]['created_utc']\n",
    "\n",
    "train_df = df[df['created_utc'] < split_date]\n",
    "test_df = df[df['created_utc'] >= split_date]\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfde657f6a25433dbb3ee925a52bec73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1904507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py:1348\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[39myield\u001b[39;00m queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m)\n\u001b[1;32m   1349\u001b[0m \u001b[39mexcept\u001b[39;00m Empty:\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/managers.py:818\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m conn\u001b[39m.\u001b[39msend((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id, methodname, args, kwds))\n\u001b[0;32m--> 818\u001b[0m kind, result \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mrecv()\n\u001b[1;32m    820\u001b[0m \u001b[39mif\u001b[39;00m kind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m#RETURN\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py:258\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 258\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[1;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39mloads(buf\u001b[39m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py:422\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 422\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    423\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py:387\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mwhile\u001b[39;00m remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     chunk \u001b[39m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    388\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[39mreturn\u001b[39;00m ds\n\u001b[1;32m     14\u001b[0m \u001b[39m# test_ds = to_dataset(test_df)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_ds \u001b[39m=\u001b[39m to_dataset(train_df)\n\u001b[1;32m     16\u001b[0m \u001b[39m# df.iloc[:2]['formatted_text'].astype(str)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mto_dataset\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_dataset\u001b[39m(df):\n\u001b[1;32m     10\u001b[0m   ds \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfrom_pandas(df[[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mformatted_text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlog_score\u001b[39m\u001b[39m'\u001b[39m]], preserve_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m   ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mmap(tokenize, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mformatted_text\u001b[39;49m\u001b[39m'\u001b[39;49m], num_proc\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m   \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3180\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3172\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3173\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3174\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3175\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3178\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3179\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3180\u001b[0m     \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3181\u001b[0m         pool, Dataset\u001b[39m.\u001b[39m_map_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3182\u001b[0m     ):\n\u001b[1;32m   3183\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3184\u001b[0m             shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py:1354\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m     \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1354\u001b[0m     [async_result\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py:1354\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m     \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1354\u001b[0m     [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py:770\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[0;32m--> 770\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_success:\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "def tokenize(batch):\n",
    "  return tokenizer(batch['formatted_text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "def to_dataset(df):\n",
    "  df = df[['id', 'formatted_text', 'log_score']].rename(columns={'log_score': 'labels'})\n",
    "  ds = Dataset.from_pandas(, preserve_index=False)\n",
    "  ds = ds.map(tokenize, batched=True, batch_size=500, remove_columns=['formatted_text'], num_proc=4)\n",
    "  return ds\n",
    "\n",
    "# test_ds = to_dataset(test_df)\n",
    "train_ds = to_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1a348be930499d9e1b4f581d4b251f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b350adf9f041ef881be62b4d7fc51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined = DatasetDict({'train': train_ds, 'test': test_ds})\n",
    "combined.save_to_disk('/workspace/data/reddit/submissions/RS_2023-01-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True).to_feather('/workspace/data/reddit/submissions/RS_2023-01-train.arrow')\n",
    "test_df.reset_index(drop=True).to_feather('/workspace/data/reddit/submissions/RS_2023-01-test.arrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
